# АНАЛИЗ ДАННЫХ И ИСКУССТВЕННЫЙ ИНТЕЛЛЕКТ [in GameDev]
Отчет по лабораторной работе #1 выполнил:
- Дубровский Давид Русланович
- РИ-210912
Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 40 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.
- ✨Magic ✨

## Цель работы


## Задание 1 
### Измените параметры файла. yaml-агента и определить какие параметры и
как влияют на обучение модели.

дефолтные графики

![1fu](https://user-images.githubusercontent.com/92369801/205155405-18465026-8cd3-4ae2-8d62-12337c2cedf4.jpg)

![2fu](https://user-images.githubusercontent.com/92369801/205155420-55f62e5b-99dc-4da6-bb04-29ab3aa21af7.jpg)

time_horizon - этот параметр является компромиссом между менее предвзятой, но более высокой оценкой дисперсии (длительный временной горизонт) и более предвзятой, но менее разнообразной оценкой

при увеличении этого параметров график вознаграждения становися более изменчивым

![new11](https://user-images.githubusercontent.com/92369801/205155335-2b3c3332-8178-4c5f-9ee5-b7d698381c12.jpg)

![new1](https://user-images.githubusercontent.com/92369801/205155224-ca370fbf-5340-4f16-924c-61d780b83860.jpg)

при уменьшении параметра learning_rate вознаграждение не увеличивается последовательно

![new2](https://user-images.githubusercontent.com/92369801/205160527-177e7f62-0fd1-4bff-ba3c-ff1a441b8279.jpg)

![new22](https://user-images.githubusercontent.com/92369801/205160560-61474859-b60b-436d-a7d3-f87b6ec9d066.jpg)

buffer_size - размер буфера, больший размер буфера соответствует более стабильным обновлениям обучения

пример с маленьким буфером - графики нестабильные

![new3](https://user-images.githubusercontent.com/92369801/205162076-9a326c80-bcc0-4863-bc17-8a1cd4c73819.jpg)

![new33](https://user-images.githubusercontent.com/92369801/205162089-9eb7ca18-4b68-414b-a677-e1f7921a0fdd.jpg)

beta - чем меньше бэта тем больше энтропия




## Задание 2
### Опишите результаты, выведенные в TensorBoard. 

![куыде3](https://user-images.githubusercontent.com/92369801/205165567-f99fd147-6b88-4f28-b4f3-aba0c2dc00fd.jpg)


Первый график вознаграждения по идее должен постепенно возрастать, при выставлении параметров для постепенности вознаграждения и если вознаграждение сразу не упрется в единицу

value loss - постепенно уменьшающееся значение потерь что и логично в при обучении модели

Policy loss - также постепенное уменьшение наблюдается так как по мере обучения модель должна принимать все более устаканенные значения

## Выводы

В ходе лабораторной работы вспомнил Google Colab и познакомился Unity. Опробовал библиотеки NumPy и Matplotlib, построил первую модель и узнал как она работает.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
